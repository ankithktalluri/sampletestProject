<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0" uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>Visualize your Apache Kafka Streams using the Quarkus Dev UI</title><link rel="alternate" href="https://developers.redhat.com/articles/2021/12/07/visualize-your-apache-kafka-streams-using-quarkus-dev-ui" /><author><name>Daniel Oh</name></author><id>2667143c-ad01-4773-80dd-4d708894a815</id><updated>2021-12-07T07:00:00Z</updated><published>2021-12-07T07:00:00Z</published><summary type="html">&lt;p&gt;This article shows how you can visualize &lt;a href="https://kafka.apache.org/documentation/streams/"&gt;Apache Kafka Streams&lt;/a&gt; with reactive applications using the Dev UI in &lt;a href="https://developers.redhat.com/products/quarkus/overview"&gt;Quarkus&lt;/a&gt;. Quarkus, a &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java&lt;/a&gt; framework, provides an extension to utilize the Kafka Streams API and also lets you implement stream processing applications based directly on &lt;a href="https://developers.redhat.com/topics/kafka-kubernetes"&gt;Kafka&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Reactive messaging and Apache Kafka&lt;/h2&gt; &lt;p&gt;With the rise of &lt;a href="https://developers.redhat.com/topics/event-driven/"&gt;event-driven architectures&lt;/a&gt;, many developers are adopting reactive programming to write business applications. The requirements for these applications literally specify that they not be processed in real-time because end users don't really expect synchronous communication experiences through web browsers or mobile devices. Instead, low latency is a more important performance criterion, regardless of data volume or concurrent users.&lt;/p&gt; &lt;p&gt;You might be wondering how reactive programming could meet this very different goal. The secret is an asynchronous communication protocol that decouples senders from the applications that consume and process events. In this design, a caller (e.g., end user) sends a message to a recipient and then keeps processing other requests without waiting for the reply. Asynchronous processing can also improve high-volume data performance, security, and scalability.&lt;/p&gt; &lt;p&gt;However, it's not easy to implement everything involved in asynchronous communication capabilities with just reactive programming. This is the reason that message-queue platforms have also come to occupy a critical role in event-driven applications. &lt;a href="https://kafka.apache.org/"&gt;Apache Kafka&lt;/a&gt; is one of the most popular platforms for processing event messages asynchronously to support reactive applications. &lt;a href="https://kafka.apache.org/documentation/streams/"&gt;Kafka Streams&lt;/a&gt; is a client library that abstracts changing event data sets (also known as &lt;em&gt;streams&lt;/em&gt;) continuously in Kafka clusters to support high throughput and scalability. A stream is a collection of data records in the form of key-value pairs.&lt;/p&gt; &lt;h2&gt;Example: Using the Quarkus Dev UI&lt;/h2&gt; &lt;p&gt;Take a look at the following &lt;code&gt;getMetaData()&lt;/code&gt; method to see how Quarkus lets you issue interactive queries to Kafka Streams using a &lt;code&gt;KafkaStreams&lt;/code&gt; injection. Find the &lt;a href="https://github.com/quarkusio/quarkus-quickstarts/blob/main/kafka-streams-quickstart/aggregator/src/main/java/org/acme/kafka/streams/aggregator/streams/InteractiveQueries.java"&gt;complete code&lt;/a&gt; in the &lt;a href="https://github.com/quarkusio/quarkus-quickstarts/blob/main/kafka-streams-quickstart"&gt;Quarkus Kafka Streams Quickstart&lt;/a&gt;.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt; @Inject KafkaStreams streams; public List&lt;PipelineMetadata&gt; getMetaData() { return streams.allMetadataForStore(TopologyProducer.WEATHER_STATIONS_STORE) .stream() .map(m -&gt; new PipelineMetadata( m.hostInfo().host() + ":" + m.hostInfo().port(), m.topicPartitions() .stream() .map(TopicPartition::toString) .collect(Collectors.toSet()))) .collect(Collectors.toList()); }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Kafka Streams also lets you build a process topology that represents a graph of sources, processors, and sinks in Kafka topics. Of course, you can monitor the streams using command-line tools (such as &lt;a href="https://docs.confluent.io/platform/current/app-development/kafkacat-usage.html"&gt;kcat&lt;/a&gt;), but the text-based output doesn't make it easy to understand how the streams are processing and consuming messages across Kafka topics.&lt;/p&gt; &lt;p&gt;Take a look at another example. The &lt;code&gt;buildTopology()&lt;/code&gt; method lets you build the stream's topology. Find the &lt;a href="https://github.com/quarkusio/quarkus-quickstarts/blob/main/kafka-streams-quickstart/aggregator/src/main/java/org/acme/kafka/streams/aggregator/streams/TopologyProducer.java"&gt;complete code&lt;/a&gt; in the &lt;a href="https://github.com/quarkusio/quarkus-quickstarts/blob/main/kafka-streams-quickstart"&gt;Quarkus Kafka Streams Quickstart&lt;/a&gt;.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt; @Produces public Topology buildTopology() { StreamsBuilder builder = new StreamsBuilder(); ObjectMapperSerde&lt;WeatherStation&gt; weatherStationSerde = new ObjectMapperSerde&lt;&gt;(WeatherStation.class); ObjectMapperSerde&lt;Aggregation&gt; aggregationSerde = new ObjectMapperSerde&lt;&gt;(Aggregation.class); KeyValueBytesStoreSupplier storeSupplier = Stores.persistentKeyValueStore(WEATHER_STATIONS_STORE); GlobalKTable&lt;Integer, WeatherStation&gt; stations = builder.globalTable( WEATHER_STATIONS_TOPIC, Consumed.with(Serdes.Integer(), weatherStationSerde)); builder.stream( TEMPERATURE_VALUES_TOPIC, Consumed.with(Serdes.Integer(), Serdes.String())) .join( stations, (stationId, timestampAndValue) -&gt; stationId, (timestampAndValue, station) -&gt; { String[] parts = timestampAndValue.split(";"); return new TemperatureMeasurement(station.id, station.name, Instant.parse(parts[0]), Double.valueOf(parts[1])); }) .groupByKey() .aggregate( Aggregation::new, (stationId, value, aggregation) -&gt; aggregation.updateFrom(value), Materialized.&lt;Integer, Aggregation&gt; as(storeSupplier) .withKeySerde(Serdes.Integer()) .withValueSerde(aggregationSerde)) .toStream() .to( TEMPERATURES_AGGREGATED_TOPIC, Produced.with(Serdes.Integer(), aggregationSerde)); return builder.build(); } &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Visualize the Kafka Streams topology&lt;/h3&gt; &lt;p&gt;To visualize the Kafka Streams topology, developers traditionally needed additional visualizer tools that run in the cloud or local development environments separately from Kafka clusters. But Quarkus's built-in Dev UI lets you see all the extensions currently loaded with relevant documentation. When you run Quarkus Dev Mode (e.g., &lt;code&gt;./mvnw quarkus:dev&lt;/code&gt;) and add a &lt;code&gt;quarkus-kafka-streams&lt;/code&gt; extension in a project, the Dev UI shows the Apache Kafka Streams extension graphically (Figure 1).&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/dev.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/dev.png?itok=RF3UnaLx" width="1440" height="782" alt="The Developer UI shows the Apache Kafka Streams extension, with a Topology button." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1. The Developer UI shows the Apache Kafka Streams extension, with a Topology button. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;When you click on the &lt;strong&gt;Topology&lt;/strong&gt; icon, it brings you to the Kafka Streams topology UI (Figure 2).&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/topic_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/topic_0.png?itok=BU-xIRU6" width="1440" height="966" alt="The Topology screen for Apache Kafka Streams shows details, including active topics." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2. The Topology screen for Apache Kafka Streams shows details, including active topics. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;The topology UI shows how the event streams sink in topics (e.g., &lt;code&gt;temperature-values&lt;/code&gt;) and how Quarkus applications consume the streams from the topics. Also, you can understand how the application eventually aggregates streams from multiple topics (&lt;code&gt;temperature-values&lt;/code&gt; and &lt;code&gt;weather-stations&lt;/code&gt;) to one topic (&lt;code&gt;temperatures-aggregated&lt;/code&gt;). The Topology UI also showcases the sequences on how the streams are sourced, joined, and aggregated continuously in Kafka clusters.&lt;/p&gt; &lt;h2&gt;Where to learn more&lt;/h2&gt; &lt;p&gt;This article has shown how to visualize Kafka Streams with Quarkus applications and the Dev UI. Quarkus also provides awesome features to improve your productivity through &lt;a href="https://quarkus.io/guides/continuous-testing"&gt;continuous testing&lt;/a&gt;, the &lt;a href="https://quarkus.io/guides/cli-tooling"&gt;Quarkus command-line interface (CLI)&lt;/a&gt;, and &lt;a href="https://quarkus.io/guides/dev-services"&gt;Dev Services&lt;/a&gt;. To learn more about Kafka and reactive messaging programming, see the following articles:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://quarkus.io/guides/kafka-reactive-getting-started"&gt;Getting Started to SmallRye Reactive Messaging with Apache Kafka&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/topics/kafka-kubernetes"&gt;How do I run Apache Kafka on Kubernetes?&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/devnation/tech-talks/gaming-telemetry-kafka"&gt;Level-up your gaming telemetry using Kafka Streams&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/devnation/tech-talks/dual-writes-kafka-debezium"&gt;Outbox pattern with OpenShift Streams for Apache Kafka and Debezium&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/devnation/tech-talks/kafka-at-edge"&gt;Kafka at the Edge: an IoT scenario with OpenShift Streams for Apache Kafka&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/12/07/visualize-your-apache-kafka-streams-using-quarkus-dev-ui" title="Visualize your Apache Kafka Streams using the Quarkus Dev UI"&gt;Visualize your Apache Kafka Streams using the Quarkus Dev UI&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Daniel Oh</dc:creator><dc:date>2021-12-07T07:00:00Z</dc:date></entry><entry><title type="html">OptaPlanner documentation turns over a new leaf</title><link rel="alternate" href="https://blog.kie.org/2021/12/optaplanner-documentation-turns-over-a-new-leaf.html" /><author><name>rsynek</name></author><id>https://blog.kie.org/2021/12/optaplanner-documentation-turns-over-a-new-leaf.html</id><updated>2021-12-07T00:00:00Z</updated><content type="html">  For years, has been offering the documentation in two formats: single-page HTML and PDF. This now changes with the launch a new documentation website, built using . What’s so exciting about the new documentation? First and foremost, it loads instantly as opposed to the old single-page HTML documentation. For example, if I want to read about repeated planning, I open the single-page HTML docs and wait nearly half a minute for the page to load, despite having a very good cable connection. With the new documentation, it took me only 2 seconds as each chapter has its own HTML page and thus the content that has to be loaded is limited. This also means it’s now easier to effectively share links to a particular section. Second, now you can search through the entire docs: The search box at the top of the page shows suggestions as soon as you start typing. Each suggestion consists of the chapter and a link to where the search term occurs. Third, if you spot a discrepancy in the documentation and would like to improve it, contributing was never easier: Finally, this new documentation website is much friendlier to search engines, which should make it show up in Google search results more often than before. BUILDING THE DOCUMENTATION WEBSITE The documentation sources remain in the , but the website assembly, named optaplanner-website-docs, became a part of the : Similarly to the entire optaplanner-website, the optaplanner-website-docs is built using Maven. The Maven module acts as a wrapper over Antora, which generates the static site from AsciiDoc sources. There are two Antora playbooks referring to documentation sources. The first one, used by default, is antora-playbook.yml that refers to the latest OptaPlanner release. ... content: edit_url: '{web_url}/edit/main/{path}' sources: - url: git@github.com:kiegroup/optaplanner.git # Updates with every release to point to the latest release branch. branches: [8.12.x] start_path: optaplanner-docs/src ... To render the latest documentation: 1. cd optaplanner-website/optaplanner-website-docs 2. mvn clean package 3. Open the index.html located in target/website/docs in your browser. The second Antora playbook, antora-playbook-author.yml, is activated by the author maven profile and refers to the current optaplanner local Git repository HEAD. ... content: edit_url: '{web_url}/edit/main/{path}' sources: # Assuming the optaplanner local repository resides next to the optaplanner-website. - url: ../../optaplanner branches: [HEAD] start_path: optaplanner-docs/src ... To preview local changes in the documentation: 1. Make sure the optaplanner and optaplanner-website Git repositories are located in the same directory or change the local URL accordingly. 2. cd optaplanner-website/optaplanner-website-docs 3. mvn clean package -Pauthor 4. Open the index.html located in target/website/docs in your browser. SUPPORTED DOCUMENTATION FORMATS Introducing new formats does not have to result in abandoning the old ones, and in our case it does not. While I encourage everyone to visit the new documentation website, the existing formats continue to be published with every release as before. Also, should you need to have a look at a particular version of the documentation, it’s still at your hand in the . CONCLUSION Since the 8.12.0.Final release, there is a new documentation website available under . The documentation is now structured into pages by chapters and searchable. The single-page HTML and PDF documentation remains available for every release. ") The post appeared first on .</content><dc:creator>rsynek</dc:creator></entry><entry><title>Boost Apache Camel performance on Quarkus</title><link rel="alternate" href="https://developers.redhat.com/articles/2021/12/06/boost-apache-camel-performance-quarkus" /><author><name>Bruno Meseguer</name></author><id>cce5069d-e1a3-4e0b-a455-5ebc6a686040</id><updated>2021-12-06T07:00:00Z</updated><published>2021-12-06T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://camel.apache.org/camel-quarkus/"&gt;Camel Quarkus&lt;/a&gt; is a subproject in the &lt;a href="https://camel.apache.org/"&gt;Apache Camel&lt;/a&gt; community that enables Camel to run on &lt;a href="https://developers.redhat.com/products/quarkus/overview"&gt;Quarkus&lt;/a&gt;. Apache Camel is the most popular open source community project aimed at solving all things integration. Quarkus is a &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java&lt;/a&gt; framework tailored for &lt;a href="https://openjdk.java.net/groups/hotspot/"&gt;OpenJDK HotSpot&lt;/a&gt; and &lt;a href="https://www.graalvm.org/"&gt;GraalVM&lt;/a&gt;, boasting lightning-fast boot times and low memory utilization.&lt;/p&gt; &lt;p&gt;This article explains how Camel has evolved over time and why it is now embracing Quarkus. I've included a quick getting started guide that will show you how easy it is to create a Camel Quarkus project and experience the significant performance benefits for yourself.&lt;/p&gt; &lt;h2&gt;Why Apache Camel is special&lt;/h2&gt; &lt;p&gt;Apache Camel has been around for several years and predates the &lt;a href="https://developers.redhat.com/topics/containers"&gt;containers&lt;/a&gt; and &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; era. The great advantage of Apache Camel against other integration technologies is that it was always kept very lightweight. To put it simply, it's a small Java library you include in a Java project.&lt;/p&gt; &lt;p&gt;Its lightness has allowed it to adjust to emerging trends quickly and fit into different runtimes (e.g., web containers, Java EE servers, &lt;a href="https://www.osgi.org/"&gt;OSGi&lt;/a&gt;, &lt;a href="https://developers.redhat.com/topics/spring-boot/"&gt;Spring Boot&lt;/a&gt;). Apache Camel keeps evolving and improving and shows how exceptionally well it withstands the test of time. It brings the enormous advantage of offering proven functionality built and polished over many years.&lt;/p&gt; &lt;p&gt;While other integration technologies were born, found initial success, and stagnated, the Apache Camel community keeps sailing ahead, reading changes in the winds well, keeping the boat tidy, and evolving new hulls and propellers when necessary (Figure 1).&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Apache Camel's sailing boat navigating changes." data-entity-type="file" data-entity-uuid="90c3ec7b-3ca9-4e20-aa3b-48d45d10e148" height="301" src="https://developers.redhat.com/sites/default/files/inline-images/sail-boat_0.jpg" width="336" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 1. Apache Camel's sailing boat navigating changes.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Java memory usage in containers&lt;/h2&gt; &lt;p&gt;Before Kubernetes and containers, it made sense to bundle services together inside a running JVM. Many Java engines (some known as enterprise service buses or ESBs) aggregated many integration processes to economize on infrastructure and memory consumption.&lt;/p&gt; &lt;p&gt;With the emergence of containers, integrations started moving from large integration servers to independent running applications and known as &lt;em&gt;&lt;a href="https://developers.redhat.com/topics/microservices"&gt;microservices&lt;/a&gt;&lt;/em&gt;. This was all well and good, but developers soon realized that the new approach was, among other challenges, very memory-hungry.&lt;/p&gt; &lt;p&gt;New languages appeared to help minimize the memory footprint but did not bring the rich functionality and libraries that Java has collected over time. If Java was the preference in the enterprise, but memory usage was problematic, it made sense to do something about it.&lt;/p&gt; &lt;h2&gt;Welcome to Camel Quarkus&lt;/h2&gt; &lt;p&gt;Camel Quarkus brings the Camel core library (Camel framework) and its rich collection of connectors (known as Camel components) to Quarkus. While boosting the use of Java in cloud-based applications, Quarkus also allows non-Kubernetes users to run applications with maximum performance on bare metal.&lt;/p&gt; &lt;p&gt;The mission of Quarkus is to run Java on minimal memory with super startup speed, thus optimizing execution on container-based platforms and reducing running costs. The following subsections explore these goals.&lt;/p&gt; &lt;h3&gt;Performance leap&lt;/h3&gt; &lt;p&gt;Quarkus gives Apache Camel a big performance leap and outperforms any other cloud-based Java runtime where Camel runs. Among Camel Quarkus's improvements are:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Subsecond startup times.&lt;/li&gt; &lt;li&gt;Low memory footprint.&lt;/li&gt; &lt;li&gt;No warmup needed.&lt;/li&gt; &lt;li&gt;Serverless characteristics.&lt;/li&gt; &lt;li&gt;Foundations for &lt;a href="https://camel.apache.org/categories/Camel-K/"&gt;Camel K&lt;/a&gt;, which deploys Camel applications on Kubernetes.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Figure 2 compares the startup time of various Java deployments for two operations: a basic RESTful API call and a RESTful API call plus database access. Quarkus with native compilation is practically instantaneous. Quarkus with the regular Java just-in-time (JIT) compiler is still much faster than traditional Java. Quarkus in native mode stands out, taking just a few hundredths of a second to start up for a sample REST service.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/perf-boot.jpg" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/perf-boot.jpg?itok=w2sjs1iv" width="600" height="153" alt="Quarkus in native mode shows subsecond boot-up time." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2. Quarkus in native mode shows subsecond boot-up time.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Figure 3 shows memory footprints for the same operations. Quarkus in native mode again lowers consumption to very economic levels.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/perf-memory.jpg" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/perf-memory.jpg?itok=m94lEAa1" width="600" height="113" alt="Quarkus's memory footprint is also much smaller than traditional runtimes." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 3. Quarkus's memory footprint is also much smaller than traditional runtimes.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Live coding&lt;/h3&gt; &lt;p&gt;An essential aspect of efficient development is feeling comfortable and productive while coding and testing—also known as developer joy. One of the key benefits of developing integrations with Camel on Quarkus is its live coding mode. In this mode, iterative trial-and-error no longer means slow cycles of compile, package, startup, then try again. Live coding mode allows you to run the build engine and make live code changes that are instantly picked up and applied, allowing you to fail and try again quickly. For example, live code updates are very helpful when you're new to Apache Camel and are still figuring things out. Live coding will accelerate the learning process.&lt;/p&gt; &lt;p&gt;Figure 4 illustrates how live coding mode works when coding and updating an Apache Camel route.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Under Quarkus's live coding mode, each code update triggers a rerun of the new code." data-entity-type="file" data-entity-uuid="eab0fdd2-d14a-46de-9357-474bc6a70610" height="391" src="https://developers.redhat.com/sites/default/files/inline-images/dev-mode-graph_1.jpg" width="852" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 4. Under Quarkus's live coding mode, each code update triggers a rerun of the new code.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The Quarkus community has applied great expertise to simplifying code creation, which also helps developers be more productive. For example, if you're implementing an integration service, and you need to provide different configurations for different environments, Camel Quarkus allows you to define properties in one single configuration file to cover all environments.&lt;/p&gt; &lt;p&gt;Let's say your Camel route needs to integrate with a backend service. Define the Camel instruction with the following XML configuration, which indicates the target host through a parameter defined between curly braces:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-xml"&gt;&lt;to uri="http:{{api.backend1.host}}/camel/subscriber/details"/&gt;&lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Then, to configure your endpoint, specify the following properties in your configuration file. The first property defines the default (production) configuration value, and the second one sets the same property for the &lt;code&gt;dev&lt;/code&gt; environment, which is activated by default when you enable live coding mode:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;api.backend1.host = end1:8080 %dev.api.backend1.host = localhost:9000&lt;/code&gt; &lt;/pre&gt; &lt;h3&gt;Camel Quarkus on Camel 3&lt;/h3&gt; &lt;p&gt;Camel Quarkus was born with Camel 3, the latest evolution of Apache Camel. At that point, Camel became a family of projects:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Camel 3: The core framework and Swiss Army knife of integration&lt;/li&gt; &lt;li&gt;Camel Quarkus: The Camel extensions for Quarkus&lt;/li&gt; &lt;li&gt;Camel K: The lightweight serverless integration platform for Kubernetes&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;One of the main drivers for upgrading Camel from version 2 to version 3 was to give life to Camel Quarkus and Camel K, which are, as you can already guess, oriented to running Camel on containers. Although Camel K is available only in Kubernetes environments, Camel Quarkus can run both in containers and standalone.&lt;/p&gt; &lt;p&gt;Camel 3 brings many internal improvements to allow it to run on Quarkus. Notably, a lot of work was done to eliminate the use of Java reflection so that Camel can run in native mode, exploited wonderfully by Quarkus, as we have seen. Another big piece of work was to modularize the core Camel framework, which allowed a significant memory footprint reduction, translating to faster startup times and execution speed.&lt;/p&gt; &lt;h2&gt;Camel Quarkus example&lt;/h2&gt; &lt;p&gt;Now that you understand what Camel Quarkus is, let's create an easy example and taste its enormous benefits. As usual, examples can range from simple demos to more sophisticated production-ready implementations. Here the intention is to keep things simple, walking you through the first steps into Camel Quarkus.&lt;/p&gt; &lt;h3&gt;Prerequisites&lt;/h3&gt; &lt;p&gt;To replicate the instructions in this section, ensure that the following requirements are installed on your system:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;JDK 11+ (any distribution)&lt;/li&gt; &lt;li&gt;Maven 3.6.2+&lt;/li&gt; &lt;li&gt;Optional: GraalVM 21.3.0 for native compilation&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Configure the generator&lt;/h3&gt; &lt;p&gt;The &lt;a href="https://code.quarkus.redhat.com/"&gt;Quarkus code generator&lt;/a&gt; is a great place to start. Filter entries using the &lt;code&gt;camel&lt;/code&gt; tag as shown in Figure 5. The interface will automatically display all the Apache Camel extensions available for Quarkus.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Search for "camel" extensions in the Quarkus code generator." data-entity-type="file" data-entity-uuid="01bb5afd-9e35-4d05-9e29-ce1e502aa62d" height="247" src="https://developers.redhat.com/sites/default/files/inline-images/quarkus-generator-list-filter_0.jpg" width="366" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 5. Search for "camel" extensions in the Quarkus code generator.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;We will create a dummy "hello world" example using the timer component and log a simple message on the screen. We will create a couple of Camel routes, one using the Java DSL (Camel's domain-specific language for Java) and the other using the XML DSL.&lt;/p&gt; &lt;p&gt;From the list, pick the &lt;strong&gt;Camel Timer&lt;/strong&gt; and &lt;strong&gt;Camel XML&lt;/strong&gt; extensions. The interface shows what you've selected (Figure 6). Click the red button labeled &lt;strong&gt;Generate your application&lt;/strong&gt; to generate and download the project.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="In the Quarkus code generator, select the Camel Timer and Camel XML extensions." data-entity-type="file" data-entity-uuid="7784dd34-ac4d-47d5-bc3e-30acd6bbc3d1" height="149" src="https://developers.redhat.com/sites/default/files/inline-images/quarkus-generator-list-selected_0.jpg" width="362" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 6. In the Quarkus code generator, select the Camel Timer and Camel XML extensions.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Work on the Camel code&lt;/h3&gt; &lt;p&gt;Download the ZIP file, unpack the project, and add the following Java class definition containing a Camel Java DSL route to the &lt;code&gt;src/main/java/org/acme/TimerRoute.java&lt;/code&gt; file:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;package org.acme; import org.apache.camel.builder.RouteBuilder; public class TimerRoute extends RouteBuilder { @Override public void configure() throws Exception { from("timer:java?period=1000").id("route-java") .log("Hello World from Java DSL"); } }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then start the Camel Quarkus application in development mode by entering in your terminal:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;mvn clean quarkus:dev&lt;/code&gt; &lt;/pre&gt; &lt;p&gt;When Quarkus starts, you should see the following message, repeated over and over:&lt;/p&gt; &lt;pre&gt; INFO [route-java] ... Hello World from Java DSL&lt;/pre&gt; &lt;h3&gt;Modify the code&lt;/h3&gt; &lt;p&gt;Running the Quarkus Camel application in development mode allows you to make code modifications that Quarkus picks up and applies on the fly. Let's see how that works by including a second Camel route using the XML DSL. Add the following XML definition to &lt;code&gt;src/main/resources/routes.xml&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-xml"&gt;&lt;routes xmlns="http://camel.apache.org/schema/spring"&gt; &lt;route id="route-xml"&gt; &lt;from uri="timer:xml?period=1000"/&gt; &lt;log message="Hello World from XML DSL"/&gt; &lt;/route&gt; &lt;/routes&gt;&lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Edit &lt;code&gt;src/main/resources/application.properties&lt;/code&gt; to add the following property, which tells Camel where to find XML route definition:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;camel.main.routes-include-pattern = classpath:routes.xml&lt;/code&gt; &lt;/pre&gt; &lt;p&gt;When you click &lt;strong&gt;Save&lt;/strong&gt;, Quarkus detects the change and automatically restarts, adding the new Camel XML definition. The animated GIF in Figure 7 shows how the restart is triggered.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/dev-mode.gif"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/dev-mode.gif" width="800" height="366" alt="In live coding mode, Quarkus restarts on each code update with the new code definition" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 7. In live coding mode, Quarkus restarts on each code update with the new code definition.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;You should see messages for both the Java and XML DSLs in your terminal:&lt;/p&gt; &lt;pre&gt; INFO [route-java] ... Hello World from Java DSL INFO [route-xml] ... Hello World from XML DSL &lt;/pre&gt; &lt;h3&gt;Measuring performance&lt;/h3&gt; &lt;p&gt;If you look at the details in the logs, you can see how quickly Apache Camel and Quarkus restarted:&lt;/p&gt; &lt;pre&gt; ... Apache Camel 3.11.1 (camel-12) &lt;strong&gt;started in 4ms&lt;/strong&gt; ...(powered by Quarkus 2.2.3.Final-redhat-00013) &lt;strong&gt;started in 0.544s.&lt;/strong&gt; &lt;/pre&gt; &lt;p&gt;&lt;br /&gt; But the true performance achievement of Quarkus is demonstrated when you build the project in native mode. Run the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;mvn clean package -Pnative&lt;/code&gt; &lt;/pre&gt; &lt;p&gt;The build process takes a few minutes to complete. When done, run the native application with:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;./target/*-runner&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you inspect the application output carefully, you'll see the time consumed upon startup. The following log extracts show the supersonic startup time of Camel Quarkus running in native mode:&lt;/p&gt; &lt;pre&gt; ... Apache Camel 3.11.1 (camel-1) &lt;strong&gt;started in 1ms&lt;/strong&gt; ...(powered by Quarkus 2.2.3.Final-redhat-00013) &lt;strong&gt;started in 0.032s.&lt;/strong&gt; &lt;/pre&gt; &lt;p&gt;&lt;br /&gt; What about memory usage? Run the following command to inspect the resident set size (RSS), which reflects the memory allocation to the process in physical RAM:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ps -o rss,command&lt;/code&gt; &lt;/pre&gt; &lt;p&gt;The output of this command should appear something like:&lt;/p&gt; &lt;pre&gt; &lt;strong&gt;25148&lt;/strong&gt; ./code-with-quarkus-1.0.0-SNAPSHOT-runner &lt;/pre&gt; &lt;p&gt;&lt;br /&gt; The number shown in bold indicates that my RSS usage almost reaches 25MB (1024KB = 1MB).&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Quarkus is the new air wind harnessed by Apache Camel 3. Camel Quarkus is a key technological evolution that brings a new dimension to Apache Camel. Quarkus opens the doors to extremely fast and reactive cloud integrations, performance leaps, more developer joy, and the container-first ethos that Quarkus embodies.&lt;/p&gt; &lt;p&gt;Check out this &lt;a href="https://developers.redhat.com/articles/2021/05/17/integrating-systems-apache-camel-and-quarkus-red-hat-openshift"&gt;Camel Quarkus article&lt;/a&gt; for a more advanced integration example deployed in Red Hat OpenShift&lt;/p&gt; &lt;p&gt;Camel Quarkus powers Camel K. Want to learn more about developing integrations with Camel K? Read this article about how to &lt;a href="https://developers.redhat.com/articles/2021/11/24/normalize-web-services-camel-k-and-atlasmap-part-1/"&gt;normalize web services with Camel K and AtlasMap&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/12/06/boost-apache-camel-performance-quarkus" title="Boost Apache Camel performance on Quarkus"&gt;Boost Apache Camel performance on Quarkus&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Bruno Meseguer</dc:creator><dc:date>2021-12-06T07:00:00Z</dc:date></entry><entry><title>Configuring Podman for Quarkus Dev Services and Testcontainers on Linux</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/quarkus-devservices-testcontainers-podman/&#xA;            " /><author><name>Michal Jurč</name></author><id>https://quarkus.io/blog/quarkus-devservices-testcontainers-podman/</id><updated>2021-12-06T00:00:00Z</updated><published>2021-12-06T00:00:00Z</published><summary type="html">Podman is a daemonless container engine for developing, managing, and running Containers on Linux systems. Since the release of version 3, Podman allows the user to run a service emulating a Docker API provided on a Unix socket. This makes it possible for Testcontainers and Quarkus Dev Services to be...</summary><dc:creator>Michal Jurč</dc:creator><dc:date>2021-12-06T00:00:00Z</dc:date></entry><entry><title>Introduction to the Node.js reference architecture, Part 6: Choosing web frameworks</title><link rel="alternate" href="https://developers.redhat.com/articles/2021/12/03/introduction-nodejs-reference-architecture-part-6-choosing-web-frameworks" /><author><name>Bethany Griggs</name></author><id>e2d2f76a-6caf-4300-871f-2bd3349ad843</id><updated>2021-12-03T07:00:00Z</updated><published>2021-12-03T07:00:00Z</published><summary type="html">&lt;p&gt;One of the key choices you make when building an enterprise &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt; application is the web framework that will serve as its foundation. As part of our &lt;a href="https://developers.redhat.com/blog/2021/03/08/introduction-to-the-node-js-reference-architecture-part-1-overview#"&gt;Node.js reference architecture effort&lt;/a&gt;, we've pulled together many internal Red Hat and IBM teams to discuss the web frameworks they've had success with. From our meetings, we've learned that most of the developers we spoke to are still happy with Express.js. This web framework has long been considered the default for Node.js, and it holds that place in our reference architecture as well.&lt;/p&gt; &lt;p&gt;However, Express.js is considered to be in maintenance mode. Thus, as part of the process of developing the reference architecture, we analyzed some data on web framework usage to try to get an idea of what might come next. In this article, you'll learn why Express.js is still a good fit for many Node.js developers and what the future could hold.&lt;/p&gt; &lt;p&gt;As with all our Node.js reference architecture recommendations, we focus on defining a set of good and reliable default choices. Some teams will deviate from some of these recommendations based on their assessment of what best fits their use case.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Read the series so far&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt;&lt;li class="Indent1"&gt;Part 1: &lt;a href="https://developers.redhat.com/blog/2021/03/08/introduction-to-the-node-js-reference-architecture-part-1-overview" target="_blank"&gt;Overview of the Node.js reference architecture&lt;/a&gt;&lt;/li&gt; &lt;li class="Indent1"&gt;Part 2: &lt;a href="https://developer.ibm.com/languages/node-js/blogs/nodejs-reference-architectire-pino-for-logging/" target="_blank"&gt;Logging in Node.js&lt;/a&gt;&lt;/li&gt; &lt;li class="Indent1"&gt;Part 3: &lt;a href="https://developers.redhat.com/articles/2021/05/17/introduction-nodejs-reference-architecture-part-3-code-consistency" target="_blank"&gt;Code consistency in Node.js&lt;/a&gt;&lt;/li&gt; &lt;li class="Indent1"&gt;Part 4: &lt;a href="https://developers.redhat.com/articles/2021/06/22/introduction-nodejs-reference-architecture-part-4-graphql-nodejs"&gt;GraphQL in Node.js&lt;/a&gt;&lt;/li&gt; &lt;li class="Indent1"&gt;Part 5: &lt;a href="https://developers.redhat.com/articles/2021/08/26/introduction-nodejs-reference-architecture-part-5-building-good-containers"&gt;Building good containers&lt;/a&gt;&lt;/li&gt; &lt;li class="Indent1"&gt;&lt;strong&gt;Part 6&lt;/strong&gt;: Choosing web frameworks&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Why Express.js?&lt;/h2&gt; &lt;p&gt;We consider Express.js a good default choice for a number of reasons:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;It's used widely, which means that there's a lot of shared knowledge about it both externally and within our organization.&lt;/li&gt; &lt;li&gt;New users can find a significant amount of resources to help them get started.&lt;/li&gt; &lt;li&gt;It has a relatively shallow dependency tree, with many dependencies maintained by the Express.js organization.&lt;/li&gt; &lt;li&gt;It's stable—it doesn't introduce breaking changes too frequently, but still addresses security vulnerabilities as necessary.&lt;/li&gt; &lt;li&gt;It's compatible across Node.js versions.&lt;/li&gt; &lt;li&gt;It's been used widely and successfully across IBM and Red Hat, including in the &lt;a href="https://developer.ibm.com/articles/evolving-the-ibm-cloud-console-with-microservices-a-nodejs-success-story/"&gt;IBM Cloud user interface&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;From our in-depth discussions as to which web framework we should recommend as our default choice, we also learned about and documented some other recommendations when using Express.js. Here are two key tips:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Register a liveness and readiness endpoint even if you're deploying initially to &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;. These endpoints are useful in environments other than Kubernetes for problem determination and monitoring.&lt;/li&gt; &lt;li&gt;Use &lt;a href="https://www.npmjs.com/package/helmet"&gt;Helmet&lt;/a&gt; to set HTTP headers for a basic level of protection from some common attacks.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Learn more by reading the &lt;a href="https://nodeshift.dev/nodejs-reference-architecture/functional-components/webframework"&gt;full details of our web framework recommendations&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Beyond Express.js: The next generation of web frameworks&lt;/h2&gt; &lt;p&gt;While Express.js is considered a good choice of web framework today, discussions and sentiment from our meetings indicate that might not continue to be the case in the future. Express.js is considered to be in maintenance mode, not in active development, and has not seen a new major release in more than five years. Because of this slow release cadence, there is concern that the framework might not keep up with the evolution of the Node.js runtime.&lt;/p&gt; &lt;p&gt;We came away from our discussion process suspecting that in the future our default web framework recommendation will change. As a result, we spent some time digging into various metrics to see what our recommended web framework might be five years from now.&lt;/p&gt; &lt;p&gt;Before we began this investigation, we needed to define its scope. We considered web frameworks that are likely to be used to handle requests and build APIs. We intentionally kept the initial pool of potential candidates as broad as possible, and tried to focus on use cases rather than looking for like-for-like frameworks.&lt;/p&gt; &lt;p&gt;For example, in the past, combining Node.js, Express.js, and a templating engine was a popular choice for building a web application. However, today you can solve the same problem using a dedicated static site framework. There are a lot more options in today's ecosystem, and where years ago for a given use case Express.js might have been the default choice, a more specialized framework might now exist for your use case.&lt;/p&gt; &lt;p&gt;We compiled an initial list of candidates from our reference architecture group discussions, as well as from lists of top Node.js frameworks compiled by outlets like &lt;a href="https://www.simform.com/blog/best-nodejs-frameworks/"&gt;Simform&lt;/a&gt; and &lt;a href="https://hackr.io/blog/nodejs-frameworks"&gt;Hackr.io&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;Key metrics&lt;/h3&gt; &lt;p&gt;Once we had defined the candidates, we collated some key metrics for each of the frameworks, including:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Weekly downloads&lt;/li&gt; &lt;li&gt;npm dependents (that is, how many packages on npm depend on this module)&lt;/li&gt; &lt;li&gt;GitHub dependents&lt;/li&gt; &lt;li&gt;GitHub stars&lt;/li&gt; &lt;li&gt;Issues&lt;/li&gt; &lt;li&gt;Last release&lt;/li&gt; &lt;li&gt;Creation date&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Generally, the metrics across the board illustrated what we expected, as you can see in Figure 1. Express.js topped the download statistics, and saw the most dependents on both GitHub and npm.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Table of high-level metrics for each web framework" data-entity-type="file" data-entity-uuid="9a4374af-a2f7-4e1b-ab9a-7636e9a4a0a9" src="https://developers.redhat.com/sites/default/files/inline-images/high-level-metrics.jpg" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 1. High-level metrics for each web framework.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;One other key takeaway is &lt;a href="https://nextjs.org/"&gt;Next.js&lt;/a&gt;'s relatively high position on the list, even though it's much newer than some of the other frameworks.&lt;/p&gt; &lt;h3&gt;Downloads&lt;/h3&gt; &lt;p&gt;Download metrics are not particularly useful for determining popularity, as the numbers can be heavily skewed by automation (from &lt;a href="https://developers.redhat.com/topics/ci-cd/all"&gt;continuous integration&lt;/a&gt; builds, for example), and also do not include organizations that use internal npm registries or caches.&lt;/p&gt; &lt;p&gt;However, these metrics can help make the relative positions of the frameworks clear. The graph in Figure 2, based on data collected on October 14, 2021, shows weekly npm downloads by web framework. Express.js dominates as expected, and Next.js is also in a strong position.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Diagram of weekly npm downloads by web framework" data-entity-type="file" data-entity-uuid="7547e627-c46e-4c7d-8d98-b21f1d3143e1" src="https://developers.redhat.com/sites/default/files/inline-images/npm-weekly-downloads.jpg" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 2. Weekly npm downloads by web framework.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Another way of looking at npm download data is to examine the percentage share of registry downloads by module, as shown in Figure 3. This helps to account for the fact that overall registry downloads are increasing year over year. This information can be calculated using the npm API; for example, to get the total number of downloads for 2020, you can use the endpoint &lt;a href="https://api.npmjs.org/downloads/point/2021-01-01:2021-12-31"&gt;https://api.npmjs.org/downloads/point/2020-01-01:2020-12-31&lt;/a&gt;.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Diagram showing the percentage share of npm registry downloads by web framework" data-entity-type="file" data-entity-uuid="3e42a2aa-ef06-47a3-902d-1e7581f1b95b" src="https://developers.redhat.com/sites/default/files/inline-images/percent-share-registry-downloads.jpg" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 3. Percentage share of npm registry downloads by web framework.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Figure 3 shows that, over time, the number of downloads to the registry for Express.js as a proportion of the whole is declining. This doesn't necessarily indicate that Express.js usage is declining; the registry downloads might simply be becoming more spread out. We added React to our analysis as a comparative measure, and found that it's seeing a similar trend to Express.js.&lt;/p&gt; &lt;p&gt;Note that hapi is listed on the graph twice—the scoped and unscoped versions are treated separately.&lt;/p&gt; &lt;p&gt;In Figure 3, you can see that a number of less frequently downloaded frameworks are clumped together at the bottom of the chart. The trends here are interesting, so Figure 4 zooms in on them.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="A table of high-level metrics for each web framework (zoomed)" data-entity-type="file" data-entity-uuid="32177250-4ef5-4731-81be-a2df5383f1c3" src="https://developers.redhat.com/sites/default/files/inline-images/percent-share-registry-downloads-zoom.jpg" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 4. Table of high-level metrics for each web framework (zoomed).&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Koa is going through a decline that's similar to what we saw with React and Express.js. Interestingly, we're seeing @hapi/hapi, Fastify, Nest.js, Next.js, and Nuxt.js all increasing, likely indicating that they're gaining popularity. However, the @hapi/hapi increase might be affected by the migration from hapi, the unscoped version of the module.&lt;/p&gt; &lt;h3&gt;Open Source Security Foundation criticality scores&lt;/h3&gt; &lt;p&gt;The Open Source Security Foundation (OpenSSF) has devised a &lt;a href="https://opensource.googleblog.com/2020/12/finding-critical-open-source-projects.html"&gt;criticality score&lt;/a&gt; that can be used to assess how critical a project is to the open source ecosystem as a whole. We generated criticality scores for all of our web framework candidates, with the results shown in Figure 5.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Open Source Security Foundation Criticality Scores" data-entity-type="file" data-entity-uuid="af91d7ba-5052-40a6-b9a0-df3bd9c5387a" src="https://developers.redhat.com/sites/default/files/inline-images/ossf-criticality.jpg" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 5. Open Source Security Foundation criticality scores.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Next.js and Fastify generated the highest criticality scores, with Express.js further down the list. You can download the tool to generate these scores, along with documentation explaining more about how they're generated, from the &lt;a href="https://github.com/ossf/criticality_score"&gt;project's GitHub repository&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;Contributions&lt;/h3&gt; &lt;p&gt;Contributions and contribution activity are also useful metrics for assessing frameworks. We began by looking at the contribution graphs generated by GitHub for each of the web frameworks we considered. (You can find this data by going to the Insights view on a project's GitHub repository.)&lt;/p&gt; &lt;p&gt;The top graph in Figure 6 illustrates a common scenario for many older web frameworks: An early peak in contribution activity, followed by a tailing off in more recent years. This could indicate that the project is in a maintenance phase rather than in active development. A few of the more recent frameworks, such as Next.js, demonstrated a more consistent pattern of activity, as illustrated in the lower graph in Figure 6.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Table of GitHub contribution activity graphs" data-entity-type="file" data-entity-uuid="604284bf-67e6-4a30-a0d7-3d163aa3bee5" src="https://developers.redhat.com/sites/default/files/inline-images/github-contribution-activity.jpg" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 6. GitHub contribution activity graphs.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Another metric that we considered was the distribution of contributions, so we looked at the share of commits per user (and their associated organizations). From this data, we could infer whether a project was mostly led by a specific individual, company, or community. Anton Whalley, Technology Partner Architect at IBM, created an application to generate these metrics using the GitHub API. You can &lt;a href="https://project-stats.netlify.app/"&gt;access the application here&lt;/a&gt; or &lt;a href="https://github.com/No9/project-stats"&gt;take a look at the source code&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;As the charts in Figure 7 illustrate, some of the web frameworks we examined, like Framework 1, are mostly dominated by a single contributor. Others, like Framework 3, have a more spread out distribution of contributions.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Charts showing share of commits by GitHub user" data-entity-type="file" data-entity-uuid="e79e5656-f5dd-4f80-95fc-8ec6d135af30" src="https://developers.redhat.com/sites/default/files/inline-images/contributor-share.jpg" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 7. Share of commits per GitHub user.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The priorities of truly community-led frameworks are not controlled by a single entity, and new users are likely to be able to contribute on an equal level.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Our Node.js reference architecture recommendations are based on what teams across IBM and Red Hat have used successfully. As such, we expect that over time our recommendations will continue to evolve. We still feel that Express.js is a good default choice for a web framework today, though we're keeping in mind some of the known concerns about it.&lt;/p&gt; &lt;p&gt;At the same time, we're continuously evaluating other frameworks by looking at their metrics and considering the qualities that would favor a different candidate in the future. Ideally, a web framework should:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Be adequately maintained.&lt;/li&gt; &lt;li&gt;See regular releases.&lt;/li&gt; &lt;li&gt;Keep up-to-date with Node.js core features and changes.&lt;/li&gt; &lt;li&gt;Be community-led with open governance.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;We'll continue to review internal and client usage and success stories. There are a few promising up-and-coming web framework candidates, but no single framework has outpaced the others enough to make it our new default recommendation—yet.&lt;/p&gt; &lt;p&gt;We hope you find these recommendations useful. While you wait for the next installment in the &lt;a href="https://developers.redhat.com/blog/2021/03/08/introduction-to-the-node-js-reference-architecture-part-1-overview"&gt;Node.js reference architecture series&lt;/a&gt;, you can check out the &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture"&gt;GitHub project&lt;/a&gt; to explore sections that might be covered in future articles.&lt;/p&gt; &lt;p&gt;If you want to learn more about what Red Hat is up to on the Node.js front, you can also explore our &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js topic page&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/12/03/introduction-nodejs-reference-architecture-part-6-choosing-web-frameworks" title="Introduction to the Node.js reference architecture, Part 6: Choosing web frameworks"&gt;Introduction to the Node.js reference architecture, Part 6: Choosing web frameworks&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Bethany Griggs</dc:creator><dc:date>2021-12-03T07:00:00Z</dc:date></entry><entry><title>Quarkus Tools for IntelliJ 1.9.0 released!</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/intellij-quarkus-tools-1.9.0/&#xA;            " /><author><name>Jeff Maury (https://twitter.com/jeffmaury)</name></author><id>https://quarkus.io/blog/intellij-quarkus-tools-1.9.0/</id><updated>2021-12-03T00:00:00Z</updated><published>2021-12-03T00:00:00Z</published><summary type="html">We are very pleased to announce the 1.9.0 release of Quarkus Tools for IntelliJ. This release adds support for Quarkus streams in the new project/module Quarkus wizard and fixes compatibilities with just released IntelliJ 2021.3. Quarkus streams When a new Quarkus project/module is to be generated, it is now possible...</summary><dc:creator>Jeff Maury (https://twitter.com/jeffmaury)</dc:creator><dc:date>2021-12-03T00:00:00Z</dc:date></entry><entry><title type="html">RESTEasy Releases</title><link rel="alternate" href="https://resteasy.github.io/2021/12/02/resteasy-releases/" /><author><name /></author><id>https://resteasy.github.io/2021/12/02/resteasy-releases/</id><updated>2021-12-02T18:11:11Z</updated><dc:creator /></entry><entry><title>Anonymize data in real time with KEDA and Rook</title><link rel="alternate" href="https://developers.redhat.com/articles/2021/12/02/anonymize-data-real-time-keda-and-rook" /><author><name>Huamin Chen, Yuval Lifshitz</name></author><id>ff2fa886-e78b-43c9-83f9-1cbf4bf9cb2b</id><updated>2021-12-02T07:00:00Z</updated><published>2021-12-02T07:00:00Z</published><summary type="html">&lt;p&gt;Data privacy and data protection have become increasingly important globally. More and more jurisdictions have passed data privacy protection laws to regulate operators that process, transfer, and store data. &lt;a href="https://en.wikipedia.org/wiki/Pseudonymization"&gt;Data pseudonymization&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/Data_anonymization"&gt;anonymization&lt;/a&gt; are two common practices that the IT industry turns to in order to comply with such laws.&lt;/p&gt; &lt;p&gt;In this article, you'll learn about an &lt;a href="https://developers.redhat.com/topics/open-source"&gt;open source&lt;/a&gt; cloud-native solution architecture we developed that allows managed data service providers to anonymize data automatically and in real time.&lt;/p&gt; &lt;h2&gt;Data anonymization&lt;/h2&gt; &lt;p&gt;Pseudonymized data can be restored to its original state through a different process, whereas anonymized data cannot. Encryption is a typical way to pseudonymize data; for example, encrypted data can be restored to its original state through decryption. On the other hand, anonymization is a process that completely removes sensitive and personal information from data. In Figures 1 and 2, images containing license plates and faces are anonymized via blurring to remove sensitive and personal information.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig1.jpg" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/fig1.jpg?itok=3_LvR6qU" width="496" height="142" alt="Face anonymization." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1. Face anonymization.&lt;/figcaption&gt;&lt;/figure&gt;&lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig2%20%281%29.jpg" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/fig2%20%281%29.jpg?itok=9QYhWeDb" width="684" height="142" alt="License plate anonymization." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2. License plate anonymization.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Once data has been anonymized, its use is no longer subject to the &lt;a href="https://en.wikipedia.org/wiki/Data_anonymization#GDPR_requirements"&gt;strict requirements of GDPR&lt;/a&gt;, the European Union's data protection law. In many use cases, anonymizing data facilitates that data's public exchange.&lt;/p&gt; &lt;h2&gt;Solution architecture overview&lt;/h2&gt; &lt;p&gt;As illustrated in Figure 3, this solution uses Cloud Native Computing Foundation (CNCF) projects such as &lt;a href="https://www.cncf.io/projects/rook/"&gt;Rook&lt;/a&gt; (serving as the infrastructure operator) and &lt;a href="https://www.cncf.io/projects/keda/"&gt;KEDA&lt;/a&gt; (providing a &lt;a href="https://developers.redhat.com/topics/serverless-architecture/"&gt;serverless&lt;/a&gt; framework), along with &lt;a href="https://www.rabbitmq.com/"&gt;RabbitMQ&lt;/a&gt; for its message queue, and &lt;a href="https://pjreddie.com/darknet/yolo/"&gt;YOLO&lt;/a&gt; and &lt;a href="https://opencv.org/"&gt;OpenCV&lt;/a&gt; for object detection without loss of generality. The containerized workloads, Rook, RabbitMQ, or KEDA, run on &lt;a href="https://microshift.io/"&gt;MicroShift&lt;/a&gt;, a small footprint &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;/&lt;a href="https://developers.redhat.com/products/openshift/getting-started"&gt;Red Hat OpenShift&lt;/a&gt; implementation.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/Copy%20of%20anonymization-arch-2.jpeg" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/Copy%20of%20anonymization-arch-2.jpeg?itok=AIGyWMUo" width="1440" height="671" alt="Diagram of the solution architecture." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 3. Solution architecture.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Under this architecture, a &lt;a href="https://www.redhat.com/en/technologies/storage/ceph"&gt;Ceph&lt;/a&gt; Object Gateway—also known as RADOS Gateway (RGW) bucket—provisioned by Rook, is configured as a &lt;a href="https://docs.ceph.com/en/latest/radosgw/notifications/"&gt;bucket notification&lt;/a&gt; endpoint to a RabbitMQ exchange. A KEDA RabbitMQ trigger is created to probe the queue lengths in the exchange. Once the queue length exceeds the threshold, a serverless function, implemented as a &lt;code&gt;StatefulSet&lt;/code&gt;, scales up, reads queue messages, parses bucket and object information, retrieves the object, detects regions of interest, blurs sensitive information, and replaces the original data with transformed data.&lt;/p&gt; &lt;h2&gt;Building the solution&lt;/h2&gt; &lt;p&gt;Bringing all these CNCF projects into one cohesive solution sounds complex. But it isn't in practice, thanks to the declarative, YAML-based configuration used by the different operators that make up the solution. Full instructions, scripts, and a recorded demo are &lt;a href="https://github.com/redhat-et/Real-Time-Data-Anonymization"&gt;available here&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;MicroShift&lt;/h3&gt; &lt;p&gt;First, we need to install &lt;a href="https://github.com/redhat-et/microshift"&gt;MicroShift&lt;/a&gt;. The solution would work in a fully fledged OpenShift or Kubernetes cluster, but if you want to get it up and running quickly on your laptop, then MicroShift is your most lightweight option.&lt;/p&gt; &lt;p&gt;First, create a default &lt;code&gt;StorageClass&lt;/code&gt; and use &lt;code&gt;hostpath-provisioner&lt;/code&gt; in the MicroShift cluster:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;cat &lt;&lt; EOF | kubectl apply -f - apiVersion: v1 kind: PersistentVolume metadata: name: hostpath-provisioner spec: capacity: storage: 8Gi accessModes: - ReadWriteOnce hostPath: path: "/var/hpvolumes" EOF kubectl patch storageclass kubevirt-hostpath-provisioner -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Another option would be to run the solution on &lt;a href="https://minikube.sigs.k8s.io/docs/"&gt;minikube&lt;/a&gt;. But minikube uses a virtual machine (VM), so you would need to add an extra disk to the minikube VM before installing Rook.&lt;/p&gt; &lt;h3&gt;Rook&lt;/h3&gt; &lt;p&gt;Next, install the Rook operator:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;kubectl apply -f https://raw.githubusercontent.com/rootfs/rook/microshift-int/cluster/examples/kubernetes/ceph/crds.yaml kubectl apply -f https://raw.githubusercontent.com/rootfs/rook/microshift-int/cluster/examples/kubernetes/ceph/common.yaml kubectl apply -f https://raw.githubusercontent.com/rootfs/rook/microshift-int/cluster/examples/kubernetes/ceph/operator-openshift.yaml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note that you have to use a developer image in &lt;code&gt;operator-openshift.yaml&lt;/code&gt;, because bucket notifications in Rook are still a work in progress.&lt;/p&gt; &lt;p&gt;Before moving on, verify that the Rook operator is up and running:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;kubectl get pods -l app=rook-ceph-operator -n rook-ceph&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now you need to create a Ceph cluster. Use &lt;code&gt;cluster-test.yaml&lt;/code&gt;, because you'll be running the solution on a single node.&lt;/p&gt; &lt;pre&gt; &lt;code&gt;kubectl apply -f https://raw.githubusercontent.com/rootfs/rook/microshift-int/cluster/examples/kubernetes/ceph/cluster-test.yaml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note that you'll use a custom Ceph image in &lt;code&gt;cluster-test.yaml&lt;/code&gt; to work around the RabbitMQ plaintext password limitation.&lt;/p&gt; &lt;p&gt;Next, verify that the cluster is up and running with monitors and OSDs:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;kubectl get pods -l app=rook-ceph-mon -n rook-ceph kubectl get pods -l app=rook-ceph-osd -n rook-ceph &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Finally, you should add the object store front end to the Ceph cluster, together with a toolbox that allows you to run administrative commands:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;kubectl apply -f https://raw.githubusercontent.com/rootfs/rook/microshift-int/cluster/examples/kubernetes/ceph/object-test.yaml kubectl apply -f https://raw.githubusercontent.com/rootfs/rook/microshift-int/cluster/examples/kubernetes/ceph/toolbox.yaml &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can now verify that the RGW is up and running:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;kubectl get pods -l app=rook-ceph-osd -n rook-ceph &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;At this point, everything is set up to create the bucket with its &lt;code&gt;StorageClass&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;cat &lt;&lt; EOF | kubectl apply -f - apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: rook-ceph-delete-bucket-my-store provisioner: rook-ceph.ceph.rook.io/bucket # driver:namespace:cluster reclaimPolicy: Delete parameters: objectStoreName: my-store objectStoreNamespace: rook-ceph # namespace:cluster region: us-east-1 --- apiVersion: objectbucket.io/v1alpha1 kind: ObjectBucketClaim metadata: name: ceph-delete-bucket labels: bucket-notification-my-notification: my-notification spec: bucketName: notification-demo-bucket storageClassName: rook-ceph-delete-bucket-my-store EOF &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note that the bucket has a special label called &lt;code&gt;bucket-notification-my-notification&lt;/code&gt; with the value &lt;code&gt;my-notification&lt;/code&gt;. This indicates that notifications should be created for this bucket. The notification custom resource hasn't been created yet, but the notification will be defined for the bucket once it has been. That's the beauty of declarative programming.&lt;/p&gt; &lt;p&gt;To create the topic and notification, you'll need some details from the RabbitMQ broker, so you'll configure that next.&lt;/p&gt; &lt;h3&gt;RabbitMQ&lt;/h3&gt; &lt;p&gt;Begin by installing the RabbitMQ operator:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;kubectl apply -f https://github.com/rabbitmq/cluster-operator/releases/latest/download/cluster-operator.yml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Next, define a RabbitMQ cluster. You can use the default "hello world" cluster for the purposes of this article:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;kubectl apply -f https://raw.githubusercontent.com/rabbitmq/cluster-operator/main/docs/examples/hello-world/rabbitmq.yaml &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;There's one manual step you'll need to take to sync up definitions between RabbitMQ and the bucket notifications topic you defined in Rook. First, define the topic exchange in RabbitMQ with the same name that you used to define in the bucket notifications topic.&lt;/p&gt; &lt;pre&gt; &lt;code&gt;kubectl exec -ti hello-world-server-0 -c rabbitmq -- rabbitmqadmin declare exchange name=ex1 type=topic&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Next, get the username/password and the service name defined for the RabbitMQ cluster:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;username="$(kubectl get secret hello-world-default-user -o jsonpath='{.data.username}' | base64 --decode)" password="$(kubectl get secret hello-world-default-user -o jsonpath='{.data.password}' | base64 --decode)" service="$(kubectl get service hello-world -o jsonpath='{.spec.clusterIP}')" &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You'll use these to create the bucket notification topic, pointing to that RabbitMQ cluster.&lt;/p&gt; &lt;p&gt;You are about to send a username/password as part of the configuration of the topic, and in production, that would require a secure connection. For the sake of this demo, however, you can use a small workaround that allows you to send them as cleartext:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- ceph config set client.rgw.my.store.a rgw_allow_secrets&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now you can configure the bucket notification topic:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;cat &lt;&lt; EOF | kubectl apply -f - apiVersion: ceph.rook.io/v1 kind: CephBucketTopic metadata: name: demo spec: endpoint: amqp://${username}:${password}@${service}:5672 objectStoreName: my-store objectStoreNamespace: rook-ceph amqp: ackLevel: broker exchange: ex1 EOF&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can also configure the notification configuration (the same one from the special &lt;code&gt;ObjectbucketClaim&lt;/code&gt; label) pointing at that topic:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;cat &lt;&lt; EOF | kubectl apply -f - apiVersion: ceph.rook.io/v1 kind: CephBucketNotification metadata: name: my-notification spec: topic: demo filter: events: - s3:ObjectCreated:* EOF&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;KEDA&lt;/h3&gt; &lt;p&gt;The last component in the configuration is KEDA, a very generic framework that allows autoscaling based on events. In this example, you will use it as a serverless framework that scales its functions based on the RabbitMQ queue length.&lt;/p&gt; &lt;p&gt;The installation of KEDA is based on &lt;a href="https://developers.redhat.com/topics/helm/all"&gt;Helm&lt;/a&gt;, so you should &lt;a href="https://helm.sh/docs/intro/install/"&gt;download&lt;/a&gt; and then install Helm if you haven't done so already. Once you have, use it to install KEDA.&lt;/p&gt; &lt;pre&gt; &lt;code&gt;helm repo add kedacore https://kedacore.github.io/charts helm repo update kubectl create namespace keda helm install keda kedacore/keda --version 1.4.2 --namespace keda&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now you need to tell KEDA what to do. The KEDA framework will be polling the queue from the RabbitMQ cluster you configured in the previous steps. That queue does not exist yet, so you need to create it first:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;kubectl exec -ti hello-world-server-0 -c rabbitmq -- rabbitmqadmin declare queue name=bucket-notification-queue durable=false kubectl exec -ti hello-world-server-0 -c rabbitmq -- rabbitmqadmin declare binding source=ex1 destination_type=queue destination=bucket-notification-queue routing_key=demo&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note that the name of the routing key (&lt;code&gt;demo&lt;/code&gt;) must match the name of the bucket notification topic.&lt;/p&gt; &lt;p&gt;KEDA will also need to know the RabbitMQ username/password and service, and the function it spawns will need to know the credentials of the Ceph user so it can read the objects from the bucket and write them back after anonymizing. You already retrieved the RabbitMQ secrets in the previous step, so now you need to get the Ceph ones:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;user=$(kubectl -n rook-ceph -it deploy/rook-ceph-tools -- radosgw-admin user list | grep ceph-user |cut -d '"' -f2) aws_key_id=$(kubectl -n rook-ceph -it deploy/rook-ceph-tools -- radosgw-admin user info --uid $user | jq -r '.keys[0].access_key') aws_key=$(kubectl -n rook-ceph -it deploy/rook-ceph-tools -- radosgw-admin user info --uid $user | jq -r '.keys[0].secret_key') aws_url=$(echo -n "http://"$(kubectl get service -n rook-ceph rook-ceph-rgw-my-store -o jsonpath='{.spec.clusterIP}') |base64) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now you can configure the secrets for KEDA and the RabbitMQ scaler for KEDA:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;cat &lt;&lt; EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: rabbitmq-consumer-secret data: amqp_url: amqp://${username}:${password}@${service}:5672 --- apiVersion: v1 kind: Secret metadata: name: rgw-s3-credential data: aws_access_key: ${aws_key} aws_key_id: ${aws_key_id} aws_endpoint_url: ${aws_url} --- apiVersion: keda.k8s.io/v1alpha1 kind: ScaledObject metadata: name: rabbitmq-consumer namespace: default spec: scaleTargetRef: deploymentName: rabbitmq-consumer triggers: - type: rabbitmq metadata: host: amqp://${username}:${password}@${service}:5672 queueName: "bucket-notification-queue" queueLength: "5" authenticationRef: name: rabbitmq-consumer-trigger --- apiVersion: keda.k8s.io/v1alpha1 kind: TriggerAuthentication metadata: name: rabbitmq-consumer-trigger namespace: default spec: secretTargetRef: - parameter: host name: rabbitmq-consumer-secret key: amqp_url EOF &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Anonymizing data&lt;/h2&gt; &lt;p&gt;Finally, you're ready to actually anonymize data. This example uses the &lt;a href="https://docs.aws.amazon.com/cli/latest/userguide/install-linux.html"&gt;awscli&lt;/a&gt; tool, but you could use any other tool or application that can upload objects to Ceph.&lt;/p&gt; &lt;p&gt;awscli uses environment variables to get the user credentials needed; you can use the ones fetched in previous steps:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;export AWS_ACCESS_KEY_ID=$aws_key export AWS_SECRET_ACCESS_KEY=$aws_key_id&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Next, you need to upload the image you want to anonymize:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;aws --endpoint-url $aws_url s3 cp image.jpg s3://notification-demo-bucket/&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The KEDA logs will show the serverless function scaling up and down:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;kubectl logs -n keda -l app=keda-operator -f&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And the serverless function's logs will show how it fetches the images, blurs them, and writes them back to Ceph:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;kubectl logs -l app=rabbitmq-consumer -f&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The RADOS Gateway logs will show the upload of the image, the sending of the notification to RabbitMQ, and the serverless function fetching the image and then uploading the modified version of it:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;kubectl logs -l app=rook-ceph-rgw -n rook-ceph -f &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This open source solution, based on CNCF projects, addresses pressing needs for data privacy protection. We believe this solution can be extended to other use cases, such as malware and ransomware detection and quarantine and data pipeline automation. For instance, ransomware encrypts objects without authorization. Our solution can help detect ransomware activities by running a serverless function to detect entropy changes as soon as an object is updated or created. Similarly, a serverless function can run, scan, and quarantine newly created objects if viruses or malware are detected.&lt;/p&gt; &lt;p&gt;If you're excited about the project or have new ideas, please share your success stories at &lt;a href="https://github.com/redhat-et/Real-Time-Data-Anonymization"&gt;our GitHub repository&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/12/02/anonymize-data-real-time-keda-and-rook" title="Anonymize data in real time with KEDA and Rook"&gt;Anonymize data in real time with KEDA and Rook&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Huamin Chen, Yuval Lifshitz</dc:creator><dc:date>2021-12-02T07:00:00Z</dc:date></entry><entry><title>Automating Quarkus with GitLab</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/logicdrop-automating-quarkus-with-gitlab/&#xA;            " /><author><name>KimJohn Quinn (https://twitter.com/logicdrop)</name></author><id>https://quarkus.io/blog/logicdrop-automating-quarkus-with-gitlab/</id><updated>2021-12-02T00:00:00Z</updated><published>2021-12-02T00:00:00Z</published><summary type="html">Logicdrop provides a business automation and data intelligence platform that enables enterprises to design their own solutions and run them in the cloud. We process millions of transactions a month across a global client-base and add significant value to mission-critical processes. The Challenge Since moving to Quarkus, our productivity quickly...</summary><dc:creator>KimJohn Quinn (https://twitter.com/logicdrop)</dc:creator><dc:date>2021-12-02T00:00:00Z</dc:date></entry><entry><title type="html">Implementing the Filter EIP using Camel-k and Kogito</title><link rel="alternate" href="https://blog.kie.org/2021/12/implementing-the-filter-eip-using-camel-k-and-kogito.html" /><author><name>Sadhana Nandakumar</name></author><id>https://blog.kie.org/2021/12/implementing-the-filter-eip-using-camel-k-and-kogito.html</id><updated>2021-12-01T22:30:15Z</updated><content type="html">Apache Camel is an Open Source integration framework that supports the implementation of the Enterprise Integration patterns (EIP). The Message Filter EIP allows you to eliminate undesired messages from a channel based on a set of criteria. The pattern uses a predicate to decide if the messages should be dropped or not.  Oftentimes the predicate logic can be complex, and might need more transparency and control by the business user. For instance, let us say we have transactions coming in, and we are only interested in the high risk transactions. As you can imagine, the rules that govern the risk can be complicated. This is where a rules engine can come in handy, to provide for a business user friendly methodology to define the predicate logic.  In this article we will discuss how to implement the Filter EIP using the cloud native technologies – Kogito and Camel-K. Kogito is designed to deliver powerful capabilities for building process and rules based applications natively in the cloud on a modern container platform. Camel K is a lightweight integration framework built from Apache Camel to run natively on Kubernetes DEFINING THE TRANSACTION RISK RULES Let us first define the transaction risk rules. As you can see below, the transaction risk is based on transactionAmount, transactionCountry and merchant information. DEPLOYING THE DECISION SERVICE The kogito project for this decision can be found . Let us now deploy this DMN as a kogito decision service on openshift. For this install the kogito operator. Now let us create a kogito build using the repo above. After the Build and Deploy the kogito decision service exposes a route so that we can invoke the decision service. We can now invoke the decision service and test it. DEFINING THE CAMEL ROUTE Let us now define the camel route. We first invoke the decision service. The service returns back a payload with a boolean value indicating the risk. We then use this decision result in the predicate for deciding if this transaction needs further processing. As you can see, the complexity of deciding if the transaction is high risk is offloaded to the kogito decision layer. The entire camel route definition can be found . DEPLOYING THE CAMEL ROUTE USING CAMEL-K We will now install the Camel K operator and deploy the integration micro service. kamel install Kamel run FilterEIP.java TESTING THE INTEGRATION MICROSERVICE Now we can lookup the route that is auto generated by the REST DSL component and send in a transaction payload. We can now see that the data is now available in the kafka topic. Notice the header information, where we set the decision evaluation to decide if the transaction needs to be sent to kafka. We will now send in a request, with a low risk. Now, we can see the risk is evaluated to be false and this transaction is filtered for post processing, so we don’t see it on the kafka topic. Similarly, it is possible to plug in decision points to other EIP patterns like content routing. Check out this from Matteo Mortari about how you can use DMN decisions for content based routing in Camel. References The post appeared first on .</content><dc:creator>Sadhana Nandakumar</dc:creator></entry></feed>
